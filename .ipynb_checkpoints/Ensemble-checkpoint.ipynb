{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rachit/.local/lib/python3.7/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "####code to  upload the data and resample the data (day wise)\n",
    "from sklearn.externals import joblib\n",
    "from keras.layers import Dense ,LSTM\n",
    "from keras.models import Sequential \n",
    "from sklearn.metrics import mean_absolute_error,mean_squared_error\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers import Flatten\n",
    "from sklearn.metrics import r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "import r2\n",
    "import mane\n",
    "\n",
    "import keras.backend as K\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "\n",
    "def r2(y_true, y_pred):\n",
    "    SS_res =  K.sum(K.square(y_true-y_pred))\n",
    "    SS_tot = K.sum(K.square(y_true - K.mean(y_true)))\n",
    "    return (1 - SS_res/(SS_tot + K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "mane_loss = mane.mane"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### importing the data files\n",
    "\n",
    "x_year = joblib.load(\"x_year.pkl\")\n",
    "y_year = joblib.load(\"y_year_values.pkl\")\n",
    "\n",
    "###slicing the tiestamp from x_year\n",
    "timestamps = x_year[:,0:1]\n",
    "\n",
    "#preparing data file for rnn\n",
    "rnn_data  = np.c_[timestamps,y_year[:,0:3]]\n",
    "\n",
    "'''\n",
    "file = open(\".dat\")\n",
    "data = file.readlines()\n",
    "rnn_data=[]\n",
    "for i in data:\n",
    "    temp = i.split(\" \")\n",
    "    rnn_data.append([int(x) for x in temp])\n",
    "'''\n",
    "\n",
    "###changing the data file to dataframe \n",
    "rnn_data = pd.DataFrame(rnn_data)\n",
    "rnn_data = rnn_data.set_index([0])\n",
    "rnn_data.index = pd.to_datetime(rnn_data.index,unit = \"s\")\n",
    "\n",
    "#resampling the data hourwise\n",
    "resampler_day = rnn_data.resample(\"D\")\n",
    "day_resampled_data = resampler_day.sum()\n",
    "\n",
    "#saving the resampled file in the pickle format \n",
    "#joblib.dump(day_resampled_data,\"day_resampled_data.pkl\")\n",
    "#changing the dataframe to array\n",
    "final_day_array = np.asarray(day_resampled_data)\n",
    "\n",
    "#selecting the size of\n",
    "print(\"Enter the fraction of data to be sent to training :\") \n",
    "training_fraction = float(input())\n",
    "length = len(final_day_array)\n",
    "train_size = int(training_fraction*length)\n",
    "\n",
    "errors_saver = {}\n",
    "\n",
    "for i in range(final_day_array.shape[1]):\n",
    "    \n",
    "    copy = final_day_array[:,i:i+1]\n",
    "    sc = MinMaxScaler()\n",
    "    copy = sc.fit_transform(copy)\n",
    "    \n",
    "    #now we are going to divide into training and test set\n",
    "    x_train = copy[:train_size,0:1]\n",
    "    y_train = copy[:train_size,0:1]\n",
    "    #x_train = x_train.reshape((-1,1,1))\n",
    "    train_x = []\n",
    "    train_y = []\n",
    "    print(\"enter n steps : \")\n",
    "    n_steps = int(input())\n",
    "    list_i = 0\n",
    "    print(\"Entering loop\")\n",
    "    print(x_train.shape)\n",
    "    print(y_train.shape)\n",
    "    k=0\n",
    "    while k<x_train.shape[0]-n_steps-1:\n",
    "        train = x_train[k:k+n_steps,0]\n",
    "        train_x.append([float(x) for x in train])\n",
    "        train_y.append(y_train[k+n_steps,0])\n",
    "        k= k+1\n",
    "    \n",
    "    train_x = np.asarray(train_x)\n",
    "    train_y = np.asarray(train_y)    \n",
    "    train_x = train_x.reshape((-1,n_steps,1))\n",
    "    train_y = train_y.reshape((-1,1,1))\n",
    "    \n",
    "    x_test = copy[train_size:,:]\n",
    "    y_test = copy[train_size:,:]\n",
    "    #x_test = x_test.reshape((-1,1,1))\n",
    "    test_x = []\n",
    "    test_y = []\n",
    "    k=0\n",
    "    while k<x_test.shape[0]-n_steps-1:\n",
    "        test= x_test[k:k+n_steps,0]\n",
    "        test_x.append([x for x in test])\n",
    "        test_y.append(y_test[k+n_steps,:])\n",
    "        k = k+1\n",
    "    test_x = np.asarray(test_x)\n",
    "    test_y = np.asarray(test_y)    \n",
    "    \n",
    "    test_x = test_x.reshape((-1,n_steps,1))\n",
    "    test_y = test_y.reshape((-1,1,1))\n",
    "    \n",
    "    '''\n",
    "    regressor = Sequential()\n",
    "    regressor.add(LSTM(units = 10,activation = \"relu\",input_shape= (None,1)))\n",
    "    regressor.add(Dense(units = 1))\n",
    "    regressor.compile(optimizer = \"adam\",loss = \"mean_squared_error\",metrics = [\"accuracy\"])\n",
    "    regressor.fit(train_x,train_y,epochs = 100 ,batch_size = 7,validation_split = 0.1)\n",
    "    '''\n",
    "\n",
    "    \n",
    "    y_pred = model.predict(test_x)\n",
    "    y_pred = y_pred.reshape((-1,1))\n",
    "    test_y  = test_y.reshape((-1,1))\n",
    "    mse = mean_squared_error(test_y,y_pred)\n",
    "    rms = sqrt(mse)\n",
    "    mae = mean_absolute_error(test_y,y_pred)\n",
    "    print(\"mean_squared_error : \",mse)\n",
    "    print(\"root_mean_squared_error : \",rms)\n",
    "    print(\"mean_absolute_error : \",mae)\n",
    "    #errors_saver[\"errors_equipment{}\".format(i+1)] = {\"mean_squared_error\":mse,\"mean_absolute_error\":mae,\"root_mean_squared_error\":rms}\n",
    "    y_pred = sc.inverse_transform(y_pred)\n",
    "    test_y = sc.inverse_transform(test_y)\n",
    "\n",
    "\n",
    "    l=0\n",
    "    while l<len(test_y)-7:\n",
    "        plt.scatter([1,2,3,4,5,6,7],y_pred[l:l+7,:],color  = \"cyan\",label = \"predicted_day\")\n",
    "        #plt.savefig(\"equipment2_day(prediction){}\".format(i//288))\n",
    "        #plt.legend()\n",
    "        #plt.show()\n",
    "        plt.scatter([1,2,3,4,5,6,7],test_y[l:l+7,:],color = \"blue\",label = \"real_values\")\n",
    "        plt.legend()\n",
    "        plt.xlabel(\"day\")\n",
    "        plt.ylabel(\"power_consumption\")\n",
    "        #plt.savefig(\"equipment9_day{}.png\".format(i//24))\n",
    "        plt.show()\n",
    "        l = l+7\n",
    "    print(\"Enter 1 if you want to process next equipment and 0 if not :\")\n",
    "    decision = int(input())\n",
    "    if decision==1:\n",
    "        pass\n",
    "    else:\n",
    "        break\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub-Models:\n",
    "- Using 3 CNN_LSTMs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_cnn_lstm(x_train, y_train):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(28,28)))\n",
    "    model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(RepeatVector(1))\n",
    "    model.add(LSTM(200, activation='relu', return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(100, activation='relu')))\n",
    "    model.add(TimeDistributed(Dense(1)))\n",
    "    model.compile(loss= mane_loss, optimizer='adam',metrics = [\"accuracy\", r2])\n",
    "\n",
    "    model.fit(x_train, y_train, epochs=30, batch_size=5, verbose = 0)\n",
    "    \n",
    "    return model\n",
    "\n",
    "os.makedirs('ensemble')\n",
    "\n",
    "# fit and save models\n",
    "n_members = 3\n",
    "for i in range(n_members):\n",
    "    model = fit_cnn_lstms(x_train, y_train)\n",
    "    filename = 'ensemble/model_' + str(i + 1) + '.h5'\n",
    "    model.save(filename)\n",
    "    print('>>> Saved %s' % filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_models(n_models):\n",
    "    all_models = list()\n",
    "    for i in range(n_models):\n",
    "        # define filename for this ensemble\n",
    "        filename = 'ensemble/model_' + str(i + 1) + '.h5'\n",
    "        # load model from file\n",
    "        model = load_model(filename)\n",
    "        # add to list of members\n",
    "        all_models.append(model)\n",
    "        print('>>> loaded %s' % filename)\n",
    "    return all_models\n",
    "\n",
    "n_members = 5\n",
    "members = load_all_models(n_members)\n",
    "print('Loaded %d models' % len(members))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Evaluating the 3 CNN_LSTMS Individually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in members:\n",
    "    testy_enc = to_categorical(testy)\n",
    "    _, acc = model.evaluate(testX, testy_enc, verbose=0)\n",
    "    print('Model Accuracy: %.3f' % acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Dataset to go into XGBoost\n",
    "- Outputs from the 3 sub models flow into XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stacked_dataset(members, inputX):\n",
    "    stackX = None\n",
    "    for model in members:\n",
    "        # make prediction\n",
    "        yhat = model.predict(inputX, verbose=0)\n",
    "        # stack predictions into [rows, members, probabilities]\n",
    "        if stackX is None:\n",
    "            stackX = yhat\n",
    "        else:\n",
    "            stackX = dstack((stackX, yhat))\n",
    "\n",
    "    # flatten [rows, members x probabilities]\n",
    "    stackX = stackX.reshape((stackX.shape[0], stackX.shape[1]*stackX.shape[2]))\n",
    "    return stackX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_stacked_model(members, inputX, inputy):\n",
    "    # create dataset using ensemble\n",
    "    stackedX = stacked_dataset(members, inputX)\n",
    "    # fit standalone model\n",
    "    model = XGBClassifier(learning_rate = 0.05, n_estimators=300, max_depth=5)\n",
    "    model.fit(stackedX, inputy)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fit_stacked_model(members, test_x, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling Ensemble Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stacked_prediction(members, model, inputX):\n",
    "    # create dataset using ensemble\n",
    "    stackedX = stacked_dataset(members, inputX)\n",
    "    # make a prediction\n",
    "    yhat = model.predict(stackedX)\n",
    "    return yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = stacked_prediction(members, model, testX)\n",
    "acc = r2_score(testy, yhat)\n",
    "print('--> Stacked Test Accuracy: %.3f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "# model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(28,28)))\n",
    "# model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "# model.add(MaxPooling1D(pool_size=2))\n",
    "# model.add(Flatten())\n",
    "# model.add(RepeatVector(1))\n",
    "# model.add(LSTM(200, activation='relu', return_sequences=True))\n",
    "# model.add(TimeDistributed(Dense(100, activation='relu')))\n",
    "# model.add(TimeDistributed(Dense(1)))\n",
    "# model.compile(loss= mane_loss, optimizer='adam',metrics = [\"accuracy\", r2])\n",
    "# # fit network\n",
    "# history = model.fit(x_train, y_train, epochs=30, batch_size=5, validation_split = 0.1)\n",
    "\n",
    "# pyplot.plot(history.history['loss'])\n",
    "# pyplot.plot(history.history['val_loss'])\n",
    "# pyplot.title('model train vs validation loss')\n",
    "# pyplot.ylabel('loss')\n",
    "# pyplot.xlabel('epoch')\n",
    "# pyplot.legend(['train', 'validation'], loc='upper right')\n",
    "# pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
